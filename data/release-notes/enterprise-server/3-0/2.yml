date: '2021-03-16'
intro: The minimum infrastructure requirements have increased for {% data variables.product.prodname_ghe_server %} 3.0+. For more information, see "[About minimum requirements for GitHub Enterprise Server 3.0 and later](/admin/enterprise-management/upgrading-github-enterprise-server#about-minimum-requirements-for-github-enterprise-server-30-and-later)."
sections:
  security_fixes:
    - Packages have been updated to the latest security versions.
  bugs:
    - During a backup an error "Warning&#58; One or more storage objects were not found on the source appliance." was occurring when attempting to clean up purgeable storage objects.
    - Dependency graph failed to parse `yarn.lock` JavaScript manifest files, resulting in HTTP 500 errors in logs.
    - Disabling GitHub Actions would sometimes fail.
    - Custom pre-receive hooks weren't allowed to write to `/tmp`, preventing some scripts from running correctly.
    - Systemd journal logs were duplicated in multiple places.
    - A timezone set on GitHub Enterprise 11.10.x or earlier was reset to UTC time after upgrading to 3.0 which caused timestamps to shift in some instances.
    - Clicking "Publish your first package" in the packages sidebar on a repository would lead to an empty page.
    - A site admin could get a 500 error page while trying to view issues referenced from private repositories.
    - After disabling GitHub Packages, some organization pages would return an HTTP 500 error response.
    - Importing of repository archives from GitHub Enterprise Server that are missing repository files would fail with an error.
    - Repository [deploy keys](/developers/overview/managing-deploy-keys) were unable to be used with repositories containing LFS objects.
    - In the packages sidebar of a repository, the Docker icon was gray and a tool tip displayed "This service is deprecated".
    - Webhooks configured with a content type of `application/x-www-form-urlencoded` did not receive query parameters in the POST request body.
    - Users could dismiss a mandatory message without checking all checkboxes.
    - In some cases after upgrading from a 2.22.X instance, the web interface assets were missing and the page would not render correctly.
    - Running `ghe-config-apply` could time out with `Failure waiting for nomad jobs to apply` due to `'job' stanza not found`.
  known_issues:
    - On a freshly set up GitHub Enterprise Server without any users, an attacker could create the first admin user.
    - Custom firewall rules are not maintained during an upgrade.
    - Git LFS tracked files [uploaded through the web interface](https://github.com/blog/2105-upload-files-to-your-repositories) are incorrectly added directly to the repository.
    - Issues cannot be closed if they contain a permalink to a blob in the same repository where the file path is longer than 255 characters.
    - When "Users can search GitHub.com" is enabled with GitHub Connect, issues in private and internal repositories are not included in GitHub.com search results.
    - When maintenance mode is enabled, some services continue to be listed as "active processes". The services identified are expected to run during maintenance mode. If you experience this issue and are unsure, contact [GitHub Enterprise Support](https://enterprise.githubsupport.com/hc/en-us) or [GitHub Premium Support](https://premium.githubsupport.com/).
    - Jupyter Notebook rendering in the web UI may fail if the notebook includes non-ASCII UTF-8 characters.
    - reStructuredText (RST) rendering in the web UI may fail and instead display raw RST markup text.
    - Old builds of Pages are not cleaned up, which could fill up the user disk (`/data/user/`).
    - When deleting a branch after merging a pull request, an error message appears although the branch deletion succeeds.
    - |
      Users may experience assets such as avatars not loading, or a failure to push/pull code. This may be caused by a PID mismatch in the `haproxy-cluster-proxy` service. To determine if you have an affected instance:

      **Single instance**

      1. Run this in the [administrative shell](https://docs.github.com/en/enterprise-server/admin/configuration/accessing-the-administrative-shell-ssh) (SSH):

        ```
        if [ $(cat /var/run/haproxy-cluster-proxy.pid) -ne $(systemctl show --property MainPID --value haproxy-cluster-proxy) ]; then echo 'Main PID of haproxy-cluster-proxy does not match /var/run/haproxy-cluster-proxy.pid'; fi
        ```

      2. If it shows that there is a mismatch, reboot the instance.

      **Cluster or High Availability configuration**

      1. Run this in the [administrative shell](https://docs.github.com/en/enterprise-server/admin/configuration/accessing-the-administrative-shell-ssh) (SSH):

        ```
        ghe-cluster-each -- 'if [ $(cat /var/run/haproxy-cluster-proxy.pid) -ne $(systemctl show --property MainPID --value haproxy-cluster-proxy) ]; then echo 'Main PID of haproxy-cluster-proxy does not match /var/run/haproxy-cluster-proxy.pid'; fi'
        ```

      2. If it shows one or more nodes are affected, reboot the affected nodes.
    - When a replica node is offline in a high availability configuration, {% data variables.product.product_name %} may still route {% data variables.product.prodname_pages %} requests to the offline node, reducing the availability of {% data variables.product.prodname_pages %} for users.
